---
title: "Retrospective of Artificial Intelligence Research"
description: "History of the work that marked the development of artificial intelligence"
pubDate: 'Feb 24 2025'
heroImage: '/post1_img.png'
lang: 'en'
---

### Promising beginnings (1943-1956)


The first prominent work was carried out by Warren McCulloch and Walter Pitts (1943) on the artificial neuron. These researchers demonstrated that any mathematical function can be calculated by a neural network and that logical connections (and, or, and not) can be implemented by simple neural structures. Donald Hebb (1949) was the first to develop a simple rule for updating the strength of connections between neurons within the network. This rule remains influential in current models.


However, Alan Turing remains the most influential figure of this era and founded the principles of machine learning, genetic algorithms, and reinforcement learning. He is also famous for inventing the Turing test. He was at the forefront in postulating as early as 1950 that it would be more efficient to develop learning algorithms allowing machines to learn rather than programming them directly. However, he warned that the introduction of autonomous systems with intelligence as powerful as a human being in human societies presents a risk for the species.


### Great expectations (1952-1969)


In the 1950s, intellectual elites questioned the ability of machines to perform certain tasks. Notably, Herbert Gelernter (1959) built a system for proving theorems in geometry, which is a precursor to mathematical proof systems. During this exploratory phase, the most influential work was by Arthur Samuel on checkers. This work used precursor algorithms of reinforcement learning allowing a computer to reach a good amateur level.


### The difficulties (1966-1973)


In 1957, Herbert Simon predicted that within 10 years a machine would reach the level of a chess champion and prove a leading mathematical theorem. This prediction would be realized 40 years later. This overconfidence, which is explained by the first results of these systems, led to their failure to solve complex problems. This failure had 2 causes, the first concerns the way the problem to be solved was approached rather as a metacognitive procedure instead of engaging in a precise analysis of the task to determine a solution and develop an algorithm producing a reliable implementation of the latter.


The second cause concerns the insoluble nature of certain problems that these systems attempt to solve. Before the formulation of complexity theory, researchers thought that it would suffice to have hardware with more power and memory. Thus, researchers could not grasp the combinatorial explosion, which was the main criticism of a famous Lighthill report (1973) which, in England, led to the suspension of funding for artificial intelligence research. Only 2 universities remained with a research program in this field.


### Expert systems (1969-1986)


The problem-solving methods presented so far were based on a general search mechanism implementing a limited series of reasoning steps. These are so-called weak methods because they do not allow generalization to more complex problems. The alternative was to apply more domain-specific rules to problems, allowing more complex reasoning to be generated and typical cases belonging to the domain of expertise to be managed. This approach met with significant success with convincing examples such as the DENDRAL program in 1969 leading to investments rising from a few million in 1980 to several billion in 1988 where companies built expert systems, vision systems, robots and specialized software and hardware. However, the builders of these systems failed to keep their promises, which led the actors into a period called the AI winter. It turns out that it remains difficult to build and maintain such systems in complex domains, partly because systems based on static rules are unable to handle uncertainty or learn from experience.


### The return of neural networks (1986-present)


In the mid-1980s, different research groups reworked the backpropagation algorithm that was developed in the 1960s.


### Probabilistic reasoning and machine learning


The failure of expert systems led researchers to develop a more scientific approach based on probabilities rather than Boolean logic, letting the machine learn rather than manual coding. It thus became common to develop new theories based on rigorous theorems or solid experimental results demonstrating their reliability on concrete applications rather than on academic prototypes.


### Big data


Remarkable advances in computing power and the creation of the web allowed the construction of immense datasets. These datasets are composed of thousands of billions of words, billions of images, billions of hours of audio and video, and also more specific data such as those relating to the genome, automotive tracking, social networks, etc. This led to the development of learning algorithms taking advantage of this large quantity of data.


### Deep learning


Machine learning refers to a subdomain of deep learning which consists of employing several layers of simple and adjustable computation modules. The first experiments date from the 1970s and experienced success in the form of convolution networks proposed by Yann Lecun in the 1990s. In 2012, during the ImageNet competition, it was a deep neural network that won a large victory. However, this remarkable success is more due to the increase in computing power. Indeed, a standard processor is capable of executing 10⁹ or 10¹⁰ operations per second while these algorithms require specialized hardware (GPU, TPU or FPGA) and can process between 10¹⁴ and 10¹⁷ operations per second, mainly in the form of parallelized operations on matrices or vectors.


### Sources
[Artificial Intelligence: A Modern Approach](https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach)
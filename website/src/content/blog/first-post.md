---
title: "Retrospective des travaux sur l'intelligence artificielle"
description: "Historique des travaux ayant marqué le développement de l'intelligence artificielle"
pubDate: 'Feb 24 2025'
heroImage: '/blog-placeholder-3.jpg'
---

### Des débuts prometteurs (1943-1956)

Les premiers travaux de premier plan qui furent réalisé par Warren McCulloch and Walter Pitts (1943) portait sur le neurone artifiel. Ces chercheurs ont  démontré que toute fonction mathématique peut être caculer par un réseau de neuronnes et que les connexités logiques (et, ou et non) peuvent être implémenté  par des structures neurales simples. Donald Hebb (1949) fut le premier a élaborer une règle simple premettant de mettre à jour la force des connexions entre les neuronnes au sein du réseau. Cette règle reste influante dans les modèles actuelles.

Néanmoins Alan Turing demeure la figure la plus influente de cette époque et fonde les principes de l'apprentissage automatique, les algorithmes génétiques et  l'apprentissage par renforcement. Il fut également célèbre pour l'invention du test de Turing. Il fut à l'avant-garde en postulant dès 1950 qu'il serait plus efficace de développer des alogorithmes d'apprentissage permettant aux machines d'apprendre plutôt que de les programmer directement. Néanmoins il avertit que l'introduction de systèmes autonomes disposant d'une intelligence aussi performante d'un être humain dans les sociétés humaines présente un risque pour l'espèce.

### De grandes attentes (1952-1969)

Dans les années 50 les élites intelectuelles mettent en doute la capacité des machines a réalisé certaines tâches. Notamment Herbert Gelernter (1959) construit un système permettant de prouvé des thorèmes en géométrie, c'est un précurseur des systèmes de preuves mathématiques. Durant cette phase exploratoire les travaux les plus influents Arthur Samuel sur le jeux de dames. Ces travaux utilisant des algorithmes précurseurs de l'apprentissage par renforcement permettant à un ordianteur d'atteindre un bon niveau amateur.

### Les difficultés (1966-1973)

En 1957 Herbert Simon prédit que dans 10 ans une machine atteindrait le niveau d'un champion d'echec et de prouver un théorème mathématique de premier plan.  Cette prediction se verra réalisée 40 ans plus tard. Cet exces de confiance qui s'explique par les premiers résultats de ces systèmes et ceux-ci echouront à  résoudre des problèmes complexes. Cet echec aura 2 causes, la première porte sur la façon dont le problème à résoudre fut abordé plutôt comme une procédure métacognitives au lieu de se livrer à  une analyse précise de la tâche afin de déterminer une solution et d'élaborer un algorithme produisant une implémentation fiable de cette dernière.

La deuxième cause concerne le caractère insoluble de certains problèmes que ces systèmes tentaient de résoudre. Avant la formulation de la théorie de la  complexité les chercheurs pensaient qu'il suffirait d'avoir du matériel disposant de plus de puissance et de mémoire. Ainsi les chercheurs n'ont pas pu saisir l'explosion combinatoire fut la critique pricipale d'un célébre rapport Lighthill (1973) qui, en Angleterre, conduit à la suspension des financement  des cherches en intelligence artificiel. Il restera seulement 2 universités disposant d'un programme de recherche dans ce domaine.

### Les systèmes experts (1969-1986)

Les methodes de résolution de problèmes présentés jusqu’ici furent basé sur un mechanisme de recherche générale mettant en oeuvre une serie limité d'étapes de  raisonnement. Ce sont des méthodes dites faibles car elles ne permettent pas de généraliser à des problèmes plus complexes. L'alternative fut d'appliquer aux problèmes des règles plus spécifique au domaine de ceux-ci, permettant de générer des raisonnements plus complexes et de gérer les cas typiques appartenant au domaine d'expertise. Cette approche rencontrera un succès important avec des examples convaincants comme le programme DENDRAL en 1969 menant à des investissements passant de quelques millions en 1980 à plusieurs millards 1988 où les entreprises ont construit des systèmes experts, systèmes de vision, des robots et des logiciels et du matériel spécialisés. Néanmoins les constructeurs de ces systèmes ont échoué à tenir les promesses ce qui fera entrer les acteurs dans une période nommé l'hiver de l'IA. Il se trouve qu'il demeure difficile de construire et de maintenir de tel systèmes dans des domaines complexes, en partie parce que des systèmes basé sur des règles statiques incapables de gérer l'incertitude où apprendre de l'expérience.

### Raisonnement probabiliste et apprentissage automatique

L'échec des systèmes experts conduisa les chercheurs à développer une approche plus scientifique basé sur les probabilités plutôt que sur la logique boolénne,  laisser la machine apprendre plutôt que le codage manuel. Il devint ainsi commun de développer de nouvelles théories basées sur des théorèmes rigoureux ou des résultats expérimentaux solides démontrant leur fiabilité sur des applications concrétes plutôt que sur des prototypes académiques.

### Big data

Les avancées remarquables en termes de puissance de calcul et la creation du web permit la construction d'immenses jeux de données. Ces jeux de données sont composés de milliers de milliards de mots, de milliards d'images, des milliards d'heures d'audio et de vidéos et également des données plus spécifique comme celles relatives au génome, traçage automobile, réseaux sociaux, etc. Cela mena au développement l'algorithmes d'apprentissage tirant avantages de cette large quantité de données.

### L'apprentissage profond

L'apprentissage automatique désigne un sous domaine de l'apprentissage profond qui consiste à employer plusieurs couches de modules de calcul simple et ajustables. Les premières expériences datent des années 70 et connues du succes sous la forme des réseaux à convolution proposé par Yan Lecun dans les années 90. En 2012 lors de la compétition d'ImageNet c'est un réseau de neuronnes profond qui remporta une large victoire. Cependant ce succes remarquable tient plus à l'augmentation de la puissance de calcul. En effet un processeur standard est capable d'executer 10⁹ ou 10¹⁰ opérations par seconde alors que ces algorithmes requièrent du matériel spécialisé (GPU, TPU ou FGPA) et peuvent traiter entre 10¹⁴ et 10¹⁷ opérations par seconde, principalement sous la forme d'opérations parallèlisées sur des matrices ou vecteurs.  

### Sources
[Artificial Intelligence: A Modern Approach](https://fr.wikipedia.org/wiki/Intelligence_artificielle_:_une_approche_moderne)
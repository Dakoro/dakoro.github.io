---
title: "Rétrospective des recherches sur l'intelligence artificielle"
description: "Historique des travaux ayant marqué le développement de l'intelligence artificielle"
pubDate: 'Feb 24 2025'
heroImage: '/post1_img.png'
lang: 'fr'
---

### Des débuts prometteurs (1943-1956)


Les premiers travaux de premier plan qui furent réalisés par Warren McCulloch and Walter Pitts (1943) portait sur le neurone artificiel. Ces chercheurs ont  démontré que toute fonction mathématique peut être calculée par un réseau de neurones et que les connexités logiques (et, ou et non) peuvent être implémentées  par des structures neurales simples. Donald Hebb (1949) fut le premier à élaborer une règle simple permettant de mettre à jour la force des connexions entre les neurones au sein du réseau. Cette règle reste influente dans les modèles actuels.


Néanmoins Alan Turing demeure la figure la plus influente de cette époque et fonde les principes de l'apprentissage automatique, les algorithmes génétiques et  l'apprentissage par renforcement. Il est également célèbre pour l'invention du test de Turing. Il fut à l'avant-garde en postulant dès 1950 qu'il serait plus efficace de développer des algorithmes d'apprentissage permettant aux machines d'apprendre plutôt que de les programmer directement. Néanmoins il avertit que l'introduction de systèmes autonomes disposant d'une intelligence aussi performante d'un être humain dans les sociétés humaines présente un risque pour l'espèce.


### De grandes attentes (1952-1969)


Dans les années 50 les élites intellectuelles mettent en doute la capacité des machines à réaliser certaines tâches. Notamment Herbert Gelernter (1959) construit un système permettant de prouver des théorèmes en géométrie, c'est un précurseur des systèmes de preuves mathématiques. Durant cette phase exploratoire, les travaux les plus influents sont Arthur Samuel sur le jeu de dames. Ces travaux utilisent des algorithmes précurseurs de l'apprentissage par renforcement permettant à un ordinateur d'atteindre un bon niveau amateur.


### Les difficultés (1966-1973)


En 1957 Herbert Simon prédit que dans 10 ans une machine atteindrait le niveau d'un champion d'échec et de prouver un théorème mathématique de premier plan.  Cette prédiction se verra réalisée 40 ans plus tard. Cet excès de confiance qui s'explique par les premiers résultats de ces systèmes et ceux-ci échouent à  résoudre des problèmes complexes. Cet échec aura 2 causes, la première porte sur la façon dont le problème à résoudre fut abordé plutôt comme une procédure métacognitives au lieu de se livrer à  une analyse précise de la tâche afin de déterminer une solution et d'élaborer un algorithme produisant une implémentation fiable de cette dernière.


La deuxième cause concerne le caractère insoluble de certains problèmes que ces systèmes tentent de résoudre. Avant la formulation de la théorie de la  complexité, les chercheurs pensaient qu'il suffirait d'avoir du matériel disposant de plus de puissance et de mémoire. Ainsi les chercheurs n'ont pas pu saisir l'explosion combinatoire fut la critique principale d'un célèbre rapport Lighthill (1973) qui, en Angleterre, conduit à la suspension des financement  des recherches en intelligence artificielle. Il reste seulement 2 universités disposant d'un programme de recherche dans ce domaine.


### Les systèmes experts (1969-1986)


Les méthodes de résolution de problèmes présentés jusqu’ici furent basées sur un mécanisme de recherche générale mettant en œuvre une série limitée d'étapes de  raisonnement. Ce sont des méthodes dites faibles car elles ne permettent pas de généraliser à des problèmes plus complexes. L'alternative fut d'appliquer aux problèmes des règles plus spécifiques au domaine de ceux-ci, permettant de générer des raisonnements plus complexes et de gérer les cas typiques appartenant au domaine d'expertise. Cette approche rencontrera un succès important avec des exemples convaincants comme le programme DENDRAL en 1969 menant à des investissements passant de quelques millions en 1980 à plusieurs milliards 1988 où les entreprises ont construit des systèmes experts, systèmes de vision, des robots et des logiciels et du matériel spécialisés. Néanmoins les constructeurs de ces systèmes ont échoué à tenir les promesses ce qui fera entrer les acteurs dans une période nommée l'hiver de l'IA. Il se trouve qu'il demeure difficile de construire et de maintenir de tels systèmes dans des domaines complexes, en partie parce que des systèmes basés sur des règles statiques incapables de gérer l'incertitude où apprendre de l'expérience.


### Le retour des réseaux de neurones (1986-aujourd'hui)


Au milieu des années 80 différents groupes de recherches ont retravaillé l'algorithme de rétropropagation qui fut développé dans les années 60.


### Raisonnement probabiliste et apprentissage automatique


L'échec des systèmes experts conduit les chercheurs à développer une approche plus scientifique basée sur les probabilités plutôt que sur la logique booléenne,  laisser la machine apprendre plutôt que le codage manuel. Il devient ainsi commun de développer de nouvelles théories basées sur des théorèmes rigoureux ou des résultats expérimentaux solides démontrant leur fiabilité sur des applications concrètes plutôt que sur des prototypes académiques.


### Big data


Les avancées remarquables en termes de puissance de calcul et la création du web permit la construction d'immenses jeux de données. Ces jeux de données sont composés de milliers de milliards de mots, de milliards d'images, des milliards d'heures d'audio et de vidéos et également des données plus spécifiques comme celles relatives au génome, traçage automobile, réseaux sociaux, etc. Cela mena au développement l'algorithmes d'apprentissage tirant avantages de cette large quantité de données.


### L'apprentissage profond


L'apprentissage automatique désigne un sous domaine de l'apprentissage profond qui consiste à employer plusieurs couches de modules de calcul simple et ajustables. Les premières expériences datent des années 70 et connaissent du succès sous la forme des réseaux à convolution proposé par Yann Lecun dans les années 90. En 2012 lors de la compétition d'ImageNet c'est un réseau de neurones profond qui remporta une large victoire. Cependant ce succès remarquable tient plus à l'augmentation de la puissance de calcul. En effet un processeur standard est capable d'exécuter 10⁹ ou 10¹⁰ opérations par seconde alors que ces algorithmes requièrent du matériel spécialisé (GPU, TPU ou FPGA) et peuvent traiter entre 10¹⁴ et 10¹⁷ opérations par seconde, principalement sous la forme d'opérations parallélisées sur des matrices ou vecteurs. 


### Sources
[Artificial Intelligence: A Modern Approach](https://fr.wikipedia.org/wiki/Intelligence_artificielle_:_une_approche_moderne)
